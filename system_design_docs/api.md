<div align="center">

  <img src="https://capsule-render.vercel.app/api?type=waving&height=300&color=gradient&text=Thomas%20To&reversal=true&desc=Fullstack%20Software,%20Biomanufacturing,%20Protein%20Design&descAlignY=65&descSize=30&section=footer" width="100%"/>

  <br />

  <a href="https://thomas-to-bcheme-github-io.vercel.app/">
    <img src="https://img.shields.io/badge/Portfolio-Visit%20Live%20Site-2ea44f?style=for-the-badge&logo=vercel&logoColor=white" alt="Portfolio" />
  </a>
  <a href="src/docs/Thomas_To_Resume.pdf?raw=true">
    <img src="https://img.shields.io/badge/Resume-Download%20PDF-0078D4?style=for-the-badge&logo=adobeacrobatreader&logoColor=white" alt="Resume" />
  </a>
  <a href="https://www.linkedin.com/in/thomas-to-ucdavis/">
    <img src="https://img.shields.io/badge/LinkedIn-Connect-0077B5?style=for-the-badge&logo=linkedin&logoColor=white" alt="LinkedIn" />
  </a>
</div>

## API Documentation

This document describes the API layer for the portfolio project.

---

## Endpoint Inventory

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/chat` | POST | Streaming chat endpoint for AI assistant |

---

## `/api/chat` - AI Chat Endpoint

### Overview
A streaming chat endpoint that connects the frontend to Google Gemini API with RAG (Retrieval Augmented Generation) context.

### Request

**Method:** `POST`

**Headers:**
```
Content-Type: application/json
```

**Body:**
```json
{
  "messages": [
    {
      "role": "user",
      "content": "Tell me about Thomas's experience"
    },
    {
      "role": "assistant",
      "content": "Thomas has..."
    }
  ]
}
```

| Field | Type | Description |
|-------|------|-------------|
| `messages` | Array | Chat history with role/content pairs |
| `messages[].role` | String | Either `"user"` or `"assistant"` |
| `messages[].content` | String | Message text content |

### Response

**Content-Type:** `text/plain; charset=utf-8`

**Format:** Server-Sent Events (SSE) / Streaming text

The response streams text chunks as they are generated by the LLM.

### Error Response

**Status:** `500 Internal Server Error`

```json
{
  "error": "Server Error"
}
```

---

## Implementation Details

### Architecture Pattern
```
Route → Controller → Service
  │         │           │
  │         │           └── Google Gemini SDK
  │         └── Message transformation
  └── Request validation
```

### Message Transformation
Messages are transformed from OpenAI-style format to Google Gemini format:

```typescript
// Input (OpenAI-style)
{ role: 'user' | 'assistant', content: string }

// Output (Google Gemini)
{ role: 'user' | 'model', parts: [{ text: string }] }
```

### Streaming Implementation
Uses `ReadableStream` for chunked response delivery:

```typescript
const stream = new ReadableStream({
  async start(controller) {
    const encoder = new TextEncoder();
    for await (const chunk of result.stream) {
      controller.enqueue(encoder.encode(chunk.text()));
    }
    controller.close();
  },
});
```

---

## Environment Variables

| Variable | Required | Description |
|----------|----------|-------------|
| `GOOGLE_API_KEY` | Yes | API key for Google Gemini |

**Configuration:**
- Production: Set in Vercel Dashboard
- Local: Set in `.env.local` (gitignored)

---

## RAG Context

The AI assistant is augmented with context from:

**Source:** `src/data/AiSystemInformation.tsx`

**Content includes:**
- Resume/professional background
- Technical skills and stack
- Project descriptions
- GitHub repository details
- System architecture overview

---

## Status Code Conventions

Following `.claude/agents/api.md` guidelines:

| Code | Meaning | When Used |
|------|---------|-----------|
| `200` | Success | Streaming response initiated |
| `400` | Bad Request | Invalid JSON payload |
| `500` | Internal Server Error | API/processing failure |

---

## Rate Limiting Considerations

**Google Gemini API Limits:**
- Free tier has request/minute limits
- Streaming reduces perceived latency but consumes same quota

**Mitigation Strategies:**
- Client-side debouncing on rapid requests
- Error handling with user-friendly messages
- Consider implementing request queuing for high traffic

---

## Security

**Authentication:** None (public portfolio endpoint)

**Protections:**
- API key stored server-side only (never exposed to client)
- Input sanitization via JSON parsing
- No user data persistence

**Future Considerations:**
- Rate limiting per IP
- Request size limits
- CORS configuration for production domain only

---

## Related Files

| File | Purpose |
|------|---------|
| `src/app/api/chat/route.ts` | API route handler |
| `src/data/AiSystemInformation.tsx` | RAG context/system prompt |
| `src/components/AiGenerator.tsx` | Client-side chat interface |
| `.claude/agents/api.md` | Agent guidance for API work |
